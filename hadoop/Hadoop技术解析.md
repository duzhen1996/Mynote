# Hadoop技术梗概

## 概要

Hadoop的核心主要包括几个子项目。Hadoop common、Hadoop HDFS、以及Hadoop MapReduce。这三个部分是Hadoop最重要的三个部分。

Hadoop common是Hadoop的核心，是曾经Hadoop项目的Core部分。很多其他版块都依赖于Hadoop common。Hadoop common主要负责Hadoop的配置以及Hadoop的远程控制、序列化的机制、抽象的文件系统等等基础功能呢，也就是一些底层API的提供。

Hadoop HDFS是一个Hadoop的分布式文件系统，是一种高吞吐量的文件系统。也是支持Hadoop预算的文件系统。

Hadoop MapReduce是一种计算模型，这个计算模型分为两个阶段，一个是Map阶段，这个阶段我们将数据集上的一个个元素进行操作，生成一个个键值对这样的结果。Reduce是这个计算模型的规约阶段，就是将我们计算的一个个键值对的结果进行一个规约，最后成为一个大结果。

## Common的实现

Hadoop Common的一个非常主要的功能就是处理配置文件，Java和Windows现在都有比较成熟的配置文件处理方案，但是有意思的是Hadoop使用的是自己的一套配置文件处理方案，也拥有不同的配置文件处理规则。

Hadoop因为配置文件都不大，所以都是采用了将配置文件文本一口气读入内存，然后再解析的方式。并且在一开始就有默认的配置文件路径，比如HDFS就有hdfs-default.xml和hdfs-site.xml，而MapReduce框架中有mapred-default.xml以及mapred-site.xml两个配置文件。

Hadoop提供了大量get和set方法来进行属性的获取和设置，并且会记录上final常量属性。值得一提的是Hadoop提供Configable接口，任何实现了这个接口的对象都能被Hadoop common写到配置文件中。

************

序列化是Hadoop Common的一个重要工作。所谓序列化，就是使用字节流解决对象传送的问题。我们使用一定的序列化算法将对象中的所有信息编程二进制码。一个完整的序列化信息包含所有成员的数据成员的名字和内容。这里我们说的是Java传统的序列化方式。但是Hadoop的序列化方式与Java的序列化方式后很大的区别。

首先，因为Hadoop有比较大的通信负载，所以我们需要尽可能传送少的内容来表达相同的意思。对于Java原有的序列化方案来说，首先对于“元数据”的处理就显得太过冗长了。在Java的序列化来说，它保留了一个对象事无巨细的信息，但是很多这些信息在Java强大的反射机制下是不足为惧的，实际上我们需要的只是一个对象的类型名就好了。这里需要提一下Hadoop的writable接口，这个接口和Java的Serializable接口一样，需要序列化的东西就需要打上writable的接口，不过Hadoop和Java不同的是，Java的Serialization只要打上这个“标签”一样的声明就可以直接进行序列化的，程序员不再需要做任何事情。

```java
class TestClass implement Writable{
  ......
}
```

但是Hadoop的writable就不仅仅是这样了，实现这个接口需要我们实现两个函数，一个是write，用以将数据成员序列化写入，一种叫做readField，将序列化流中的数据库反序列化读出，而这两个方法中写入数据成员的速度和读出数据成员的顺序是一样的，这就是为什么Hadoop的序列化传可以更加简洁的另一个原因，那就是不在序列化串中声明每一个数据成员的名字，方便反序列化的时候一一赋值，而是认为规定了数据成员序列化与反序列化的顺序，从而在另一种形式上做到一一对应。

当然Hadoop也提供了现成的、高度健壮的数据序列化方案，比如基本类型的Writable改造以及ObjectWritable对象。这都是现成的。这两个都是将普通的数据类型传入他们的构造函数来初始化，然后就可以调用序列化方法了。这种现成方案的处理更加全面和现先进，实现比较复杂，但是解决了所有数据类型序列化的问题。值得一提的是，这种方式提供了基本数据类型显式变长存储的方式，是的对于带宽的占用更小了。

********

压缩也是Hadoop通信非常重要的部分，

